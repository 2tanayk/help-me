# Importing Libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings
from sklearn.metrics import classification_report,ConfusionMatrixDisplay
warnings.filterwarnings('ignore')


# Reading dataset

dataset = pd.read_csv('bank.csv')
dataset


# Correlation Matrix

correl = dataset.corr()
sns.heatmap(correl,annot=True)
plt.show()


X = dataset[['variance','skewness','curtosis']]
y = dataset['class']
X


# Splitting data into training and test set

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,
                                                    random_state=13)
X_train

# Standardization
sc = StandardScaler()
X_train[['variance','skewness','curtosis']] =  sc.fit_transform(X_train)
X_test[['variance','skewness','curtosis']]  = sc.transform(X_test)
X_train

X_test


# 3D plot of features

fig = plt.figure()
ax = plt.axes(projection ='3d')

ax.scatter(X_train['variance'],X_train['skewness'],X_train['curtosis'])
ax.set_title('3D plot of Training set')
plt.show()


Principal Component Analysis
# PCA

pca = PCA(n_components=2)
X_train_2dim = pd.DataFrame(pca.fit_transform(X_train),
                            columns = [['prin_comp1',
                                        'prin_comp2']])

X_test_2dim = pd.DataFrame(pca.transform(X_test),
                            columns = [['prin_comp1',
                                        'prin_comp2']])

X_train_1dim = X_train_2dim['prin_comp1']
X_test_1dim = X_test_2dim['prin_comp1']

X_train_2dim

X_train_1dim


# 2D plot of features

plt.scatter(X_train_2dim['prin_comp1'],X_train_2dim['prin_comp2'])
plt.title('2D plot of Training set after PCA')
plt.show()


# Combined 3D and 2D plot of features

fig = plt.figure()
ax = plt.axes(projection ='3d')

ax.scatter(X_train['variance'],X_train['skewness'],X_train['curtosis'],color='red')
ax.scatter(X_train_2dim['prin_comp1'],X_train_2dim['prin_comp2'],color='blue')
ax.set_title('Combined Plot')
plt.legend(labels=['Original 3D','After PCA 2D'])
plt.show()


# PCA Info

print('Info retained by PCA components (%) : ',
      [val*100 for val in pca.explained_variance_ratio_])
print()
print('Eigen Values : ',
      pca.explained_variance_)
print()
print('Eigen Vectors : ')
print(pca.components_)


K Nearest Neighbors
# KNN On Original Dataset

knn = KNeighborsClassifier()
knn.fit(X_train,y_train)

print('Model performance on original training set : ')
print()
print(classification_report(y_train,
                            knn.predict(X_train)))

print()
print('Model performance on original test set : ')
print()
print(classification_report(y_test,
                            knn.predict(X_test)))
print()
print('Confusion Matrix : ')
cm_display = ConfusionMatrixDisplay.from_estimator(
             knn, X_test, y_test,
             display_labels=['0','1'])
plt.show()


# KNN On 2D PCA

knn = KNeighborsClassifier()
knn.fit(X_train_2dim,y_train)

print('Model performance after 2D PCA on training set : ')
print()
print(classification_report(y_train,
                            knn.predict(X_train_2dim)))

print()
print('Model performance after 2D PCA on test set : ')
print()
print(classification_report(y_test,
                            knn.predict(X_test_2dim)))
print()
print('Confusion Matrix : ')
cm_display = ConfusionMatrixDisplay.from_estimator(
             knn, X_test_2dim, y_test,
             display_labels=['0','1'])
plt.show()


# KNN On 1D PCA

knn = KNeighborsClassifier()
knn.fit(X_train_1dim,y_train)

print('Model performance after 1D PCA on training set : ')
print()
print(classification_report(y_train,
                            knn.predict(X_train_1dim)))

print()
print('Model performance after 1D PCA on test set : ')
print()
print(classification_report(y_test,
                            knn.predict(X_test_1dim)))
print()
print('Confusion Matrix : ')
cm_display = ConfusionMatrixDisplay.from_estimator(
             knn, X_test_1dim, y_test,
             display_labels=['0','1'])
plt.show()

Comparison of Results
result = pd.DataFrame({'Original':[100,100],
                      '2D PCA':[94,90],
                      '1D PCA':[78,75]},
                      index=['Training Accuracy (%)','Testing Accuracy (%)'])
print('Comparing Performance before and after PCA : \n')
result


#######################################################



import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder , StandardScaler;
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

dataset = pd.read_csv("Social_Network_Ads.csv");

le = LabelEncoder()
dataset['Gender']  = le.fit_transform(dataset['Gender']);

dataset.Gender.fillna(np.mean(dataset['Gender']))

dataset.drop(["User ID"], axis=1, inplace=True);
X = dataset[['Gender', 'Age', 'EstimatedSalary']]
Y = dataset[['Purchased']]

sc = StandardScaler()
X_scaled = sc.fit_transform(X);

# PCA 1
pca = PCA(n_components=1);
X_1_dimension = pca.fit_transform(X_scaled)

# PCA 2
pca = PCA(n_components=2);
X_2_dimension = pca.fit_transform(X_scaled)

print(pca.n_components_)
print(pca.explained_variance_)

def score(X):
    X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size = 0.2 , random_state = 12);

    model = LogisticRegression()
    model.fit(X_train , Y_train)

    return model.score(X_test , Y_test);

print("Normal   : " , score(X_scaled))
print("PCA 1    : " , score(X_1_dimension));
print("PCA 2    : " , score(X_2_dimension));


print(pca.n_components_)
print(pca.explained_variance_)
